{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegPath Histology Image Segmentation\n",
    "\n",
    "This notebook trains a semantic segmentation model for identifying 4 cell types in H&E-stained histology images:\n",
    "1. Epithelial cells (panCK_Epithelium)\n",
    "2. Smooth muscle cells (aSMA_SmoothMuscle)\n",
    "3. Lymphocytes (CD3CD20_Lymphocyte) \n",
    "4. Myeloid cells (MNDA_MyeloidCell)\n",
    "\n",
    "We'll use the SegPath dataset, which provides paired H&E images and binary masks for each cell type. The model can then be applied to Visium HD spatial transcriptomics data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision numpy pandas matplotlib albumentations tqdm segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm.notebook import tqdm\n",
    "import segmentation_models_pytorch as smp\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check Device and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal) device for training\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using {device} for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory where your datasets are stored\n",
    "# When running locally\n",
    "base_dir = os.path.expanduser(\"~/Downloads\")\n",
    "\n",
    "# When running on GCP, modify this path\n",
    "# base_dir = \"/path/to/your/data/on/gcp\"\n",
    "\n",
    "# Define cell type directories\n",
    "cell_type_dirs = {\n",
    "    'epithelial': os.path.join(base_dir, 'panCK_Epithelium'),\n",
    "    'smooth_muscle': os.path.join(base_dir, 'aSMA_SmoothMuscle'),\n",
    "    'lymphocyte': os.path.join(base_dir, 'CD3CD20_Lymphocyte'),\n",
    "    'myeloid': os.path.join(base_dir, 'MNDA_MyeloidCell')\n",
    "}\n",
    "\n",
    "# CSV file paths\n",
    "csv_paths = {\n",
    "    'epithelial': os.path.join(base_dir, 'panCK_fileinfo.csv'),\n",
    "    'smooth_muscle': os.path.join(base_dir, 'aSMA_fileinfo.csv'),\n",
    "    'lymphocyte': os.path.join(base_dir, 'CD3CD20_fileinfo.csv'),\n",
    "    'myeloid': os.path.join(base_dir, 'MNDA_fileinfo.csv')\n",
    "}\n",
    "\n",
    "combined_csv_path = os.path.join(base_dir, 'combined_dataset.csv')\n",
    "\n",
    "# Output directory for saving models and results\n",
    "output_dir = os.path.join(base_dir, 'model_output')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Verify directories exist\n",
    "for cell_type, dir_path in cell_type_dirs.items():\n",
    "    if os.path.exists(dir_path):\n",
    "        print(f\"✓ {cell_type} directory found: {dir_path}\")\n",
    "    else:\n",
    "        print(f\"✗ {cell_type} directory not found: {dir_path}\")\n",
    "\n",
    "# Verify CSV files exist\n",
    "for cell_type, csv_path in csv_paths.items():\n",
    "    if os.path.exists(csv_path):\n",
    "        print(f\"✓ {cell_type} CSV found: {csv_path}\")\n",
    "    else:\n",
    "        print(f\"✗ {cell_type} CSV not found: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combine CSV Files from Different Cell Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Read each CSV and add a column indicating the cell type\n",
    "for cell_type, csv_path in csv_paths.items():\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['cell_type'] = cell_type\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: {csv_path} not found\")\n",
    "\n",
    "# Combine all dataframes\n",
    "if dfs:\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Save the combined dataframe\n",
    "    combined_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Combined CSV created with {len(combined_df)} entries\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(\"\\nSplit distribution:\")\n",
    "    print(combined_df['train_val_test'].value_counts())\n",
    "    \n",
    "    print(\"\\nCell type distribution:\")\n",
    "    print(combined_df['cell_type'].value_counts())\n",
    "else:\n",
    "    print(\"No valid CSV files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Implementation\n",
    "\n",
    "Create a PyTorch Dataset class to load the SegPath data with balancing between cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# StainNormalizer class for H&E stain normalization\n",
    "class StainNormalizer:\n",
    "    def __init__(self, method='macenko'):\n",
    "        try:\n",
    "            import staintools\n",
    "            self.normalizer = staintools.StainNormalizer(method=method)\n",
    "            self.is_fitted = False\n",
    "            self.available = True\n",
    "        except ImportError:\n",
    "            print(\"Warning: staintools not installed. Stain normalization will be disabled.\")\n",
    "            self.available = False\n",
    "            \n",
    "    def fit(self, reference_img):\n",
    "        \"\"\"Fit the normalizer to a reference image\"\"\"\n",
    "        if not self.available:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            self.normalizer.fit(reference_img)\n",
    "            self.is_fitted = True\n",
    "            print(\"Stain normalizer fitted successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting stain normalizer: {e}\")\n",
    "    \n",
    "    def transform(self, image):\n",
    "        \"\"\"Apply stain normalization to an image\"\"\"\n",
    "        if not self.available or not self.is_fitted:\n",
    "            return image\n",
    "        \n",
    "        try:\n",
    "            return self.normalizer.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Stain normalization failed: {e}\")\n",
    "            return image\n",
    "\n",
    "class SegPathDataset(Dataset):\n",
    "    \"\"\"Dataset for loading SegPath data with balanced sampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 csv_file, \n",
    "                 base_dir,\n",
    "                 split='train', \n",
    "                 samples_per_cell_type=2000,  # Increased sample size\n",
    "                 seed=42,\n",
    "                 transform=None,\n",
    "                 use_stain_normalization=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file: Path to the combined CSV file\n",
    "            base_dir: Base directory containing cell type folders\n",
    "            split: 'train', 'val', or 'test'\n",
    "            samples_per_cell_type: How many samples to take from each cell type\n",
    "            seed: Random seed for reproducibility\n",
    "            transform: Optional transform to apply\n",
    "            use_stain_normalization: Whether to apply stain normalization\n",
    "        \"\"\"\n",
    "        # Read the combined CSV file\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "        # Filter by the specified split\n",
    "        self.df = self.df[self.df['train_val_test'] == split]\n",
    "\n",
    "        # Only keep mask files (we'll derive HE image paths from these)\n",
    "        mask_files = self.df[self.df['filename'].str.contains('_mask.png')]\n",
    "\n",
    "        # Map cell types to numerical labels\n",
    "        self.cell_type_mapping = {\n",
    "            'epithelial': 1,\n",
    "            'smooth_muscle': 2,\n",
    "            'lymphocyte': 3,\n",
    "            'myeloid': 4\n",
    "        }\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Initialize stain normalizer if requested\n",
    "        self.use_stain_normalization = use_stain_normalization\n",
    "        self.normalizer = None\n",
    "        \n",
    "        if use_stain_normalization:\n",
    "            self.normalizer = StainNormalizer(method='macenko')\n",
    "            \n",
    "            # Find a reference image (first epithelial sample)\n",
    "            for _, row in self.df[self.df['cell_type'] == 'epithelial'].head(1).iterrows():\n",
    "                he_filename = row['filename'].replace('_mask.png', '_HE.png')\n",
    "                ref_img_path = os.path.join(base_dir, he_filename)\n",
    "                if os.path.exists(ref_img_path):\n",
    "                    ref_img = cv2.imread(ref_img_path)\n",
    "                    if ref_img is not None:\n",
    "                        ref_img = cv2.cvtColor(ref_img, cv2.COLOR_BGR2RGB)\n",
    "                        self.normalizer.fit(ref_img)\n",
    "                        break\n",
    "\n",
    "        # Sample files for each cell type\n",
    "        self.samples = []\n",
    "        self.corrupted_files = []  # Keep track of corrupted files\n",
    "\n",
    "        for cell_type in self.cell_type_mapping.keys():\n",
    "            # Get mask files for this cell type\n",
    "            cell_type_masks = mask_files[mask_files['cell_type'] == cell_type]\n",
    "\n",
    "            # Determine sample size (min of requested samples or available samples)\n",
    "            sample_size = min(samples_per_cell_type, len(cell_type_masks))\n",
    "\n",
    "            # Randomly sample rows\n",
    "            sampled_masks = cell_type_masks.sample(n=sample_size, random_state=seed)\n",
    "\n",
    "            # Add to our samples list\n",
    "            valid_samples_count = 0\n",
    "            for _, row in sampled_masks.iterrows():\n",
    "                mask_path = os.path.join(base_dir, row['filename'])\n",
    "\n",
    "                # Derive HE image path from mask path\n",
    "                he_filename = row['filename'].replace('_mask.png', '_HE.png')\n",
    "                he_path = os.path.join(base_dir, he_filename)\n",
    "\n",
    "                # Check if both files exist\n",
    "                if os.path.exists(he_path) and os.path.exists(mask_path):\n",
    "                    # Try to actually open the files to check for corruption\n",
    "                    try:\n",
    "                        # Attempt to read the image files\n",
    "                        img = cv2.imread(he_path)\n",
    "                        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "                        # If either file is corrupted, imread will return None\n",
    "                        if img is None or mask is None:\n",
    "                            self.corrupted_files.append((he_path, mask_path))\n",
    "                            continue\n",
    "                        \n",
    "                        # Also check if the image dimensions match our expectations\n",
    "                        if img.shape[0] != 984 or img.shape[1] != 984:\n",
    "                            print(f\"Warning: Unexpected image dimensions {img.shape} for {he_path}\")\n",
    "                            self.corrupted_files.append((he_path, mask_path))\n",
    "                            continue\n",
    "                        \n",
    "                        # If we made it here, the files are valid\n",
    "                        cell_type_label = self.cell_type_mapping[cell_type]\n",
    "                        self.samples.append((he_path, mask_path, cell_type_label))\n",
    "                        valid_samples_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading files {he_path} or {mask_path}: {str(e)}\")\n",
    "                        self.corrupted_files.append((he_path, mask_path))\n",
    "\n",
    "        # Print information about samples and corrupted files\n",
    "        print(f\"Loaded {len(self.samples)} samples for {split} split\")\n",
    "        print(f\"Found {len(self.corrupted_files)} corrupted files that were skipped\")\n",
    "\n",
    "        # Count samples by cell type\n",
    "        cell_type_counts = {}\n",
    "        for _, _, label in self.samples:\n",
    "            cell_type_counts[label] = cell_type_counts.get(label, 0) + 1\n",
    "\n",
    "        for label, count in cell_type_counts.items():\n",
    "            cell_type_name = [k for k, v in self.cell_type_mapping.items() if v == label][0]\n",
    "            print(f\"  {cell_type_name}: {count} samples\")\n",
    "            \n",
    "        # Set transform\n",
    "        self.transform = transform if transform is not None else self._get_default_transform(split)\n",
    "    \n",
    "    def _get_default_transform(self, phase):\n",
    "        \"\"\"Get default transforms if none are provided\"\"\"\n",
    "        if phase == 'train':\n",
    "            return A.Compose([\n",
    "                # Add padding to make dimensions divisible by 32\n",
    "                A.PadIfNeeded(min_height=992, min_width=992, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "                \n",
    "                # Spatial transforms - stronger and more frequent\n",
    "                A.RandomRotate90(p=0.7),\n",
    "                A.HorizontalFlip(p=0.7),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(\n",
    "                    shift_limit=0.1, \n",
    "                    scale_limit=0.2,  # Increased scale variation\n",
    "                    rotate_limit=30,  # Increased rotation range\n",
    "                    p=0.7            # Higher probability of applying\n",
    "                ),\n",
    "                \n",
    "                # Color augmentation - critical for H&E images\n",
    "                A.OneOf([\n",
    "                    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=25, p=0.7),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),\n",
    "                    A.CLAHE(clip_limit=4.0, p=0.7),  # Contrast Limited Adaptive Histogram Equalization\n",
    "                ], p=0.8),\n",
    "                \n",
    "                # Add some noise/blur to simulate microscopy artifacts\n",
    "                A.OneOf([\n",
    "                    A.GaussianBlur(blur_limit=3, p=0.5),\n",
    "                    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "                ], p=0.5),\n",
    "                \n",
    "                # Normalization and conversion to tensor\n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "        else:  # validation or test\n",
    "            return A.Compose([\n",
    "                # Add padding to make dimensions divisible by 32\n",
    "                A.PadIfNeeded(min_height=992, min_width=992, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "                \n",
    "                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Try to get a valid sample, with a limit on retries to avoid infinite loops\n",
    "        max_retries = 5\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Get the paths and label\n",
    "                img_path, mask_path, label = self.samples[idx]\n",
    "                \n",
    "                # Load the image and mask\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    print(f\"Warning: Failed to load image {img_path} - will try another sample\")\n",
    "                    # Try another index\n",
    "                    idx = (idx + 1) % len(self.samples)\n",
    "                    continue\n",
    "                    \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Apply stain normalization if enabled\n",
    "                if self.use_stain_normalization and self.normalizer is not None:\n",
    "                    image = self.normalizer.transform(image)\n",
    "                \n",
    "                mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if mask is None:\n",
    "                    print(f\"Warning: Failed to load mask {mask_path} - will try another sample\")\n",
    "                    # Try another index\n",
    "                    idx = (idx + 1) % len(self.samples)\n",
    "                    continue\n",
    "                \n",
    "                # Convert binary mask (0/1) to the specific cell type label\n",
    "                labeled_mask = np.zeros_like(mask)\n",
    "                labeled_mask[mask == 1] = label\n",
    "                \n",
    "                # Apply transformations\n",
    "                transformed = self.transform(image=image, mask=labeled_mask)\n",
    "                image = transformed['image']\n",
    "                labeled_mask = transformed['mask']\n",
    "                \n",
    "                return {\n",
    "                    'image': image,\n",
    "                    'mask': labeled_mask,\n",
    "                    'cell_type': label,\n",
    "                    'path': img_path\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error in __getitem__ for {idx}: {str(e)} - will try another sample\")\n",
    "                idx = (idx + 1) % len(self.samples)\n",
    "        \n",
    "        # If we've exhausted all retries, create a dummy sample\n",
    "        # This is a last resort to avoid crashing the training loop\n",
    "        print(f\"WARNING: Failed to load any valid sample after {max_retries} attempts\")\n",
    "        dummy_image = torch.zeros(3, 992, 992)  # Padded dimensions\n",
    "        dummy_mask = torch.zeros(992, 992).long()\n",
    "        \n",
    "        return {\n",
    "            'image': dummy_image,\n",
    "            'mask': dummy_mask,\n",
    "            'cell_type': 1,  # Default to epithelial\n",
    "            'path': 'dummy_path'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Some Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for each split with more samples\n",
    "train_dataset = SegPathDataset(\n",
    "    csv_file=combined_csv_path,\n",
    "    base_dir=base_dir,\n",
    "    split='train',\n",
    "    samples_per_cell_type=2000  # Increased from 500 to 2000\n",
    ")\n",
    "\n",
    "val_dataset = SegPathDataset(\n",
    "    csv_file=combined_csv_path,\n",
    "    base_dir=base_dir,\n",
    "    split='val',\n",
    "    samples_per_cell_type=300  # Increased from 100 to 300\n",
    ")\n",
    "\n",
    "test_dataset = SegPathDataset(\n",
    "    csv_file=combined_csv_path,\n",
    "    base_dir=base_dir,\n",
    "    split='test',\n",
    "    samples_per_cell_type=300  # Increased from 100 to 300\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4  # Smaller batch size for MacBook Pro\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_batch(loader, title):\n",
    "    \"\"\"\n",
    "    Visualize a batch of images and corresponding masks from a DataLoader.\n",
    "    \n",
    "    Args:\n",
    "        loader: DataLoader object.\n",
    "        title: Title for the plot.\n",
    "    \"\"\"\n",
    "    # Get a batch of samples\n",
    "    batch = next(iter(loader))\n",
    "    images = batch['image'].cpu().numpy()  # shape: [B, C, H, W]\n",
    "    masks = batch['mask'].cpu().numpy()      # shape: [B, H, W]\n",
    "    cell_types = batch['cell_type'].cpu().numpy()  # shape: [B]\n",
    "    \n",
    "    # Mapping numeric labels to names\n",
    "    cell_type_names = {\n",
    "        1: 'epithelial',\n",
    "        2: 'smooth_muscle',\n",
    "        3: 'lymphocyte',\n",
    "        4: 'myeloid'\n",
    "    }\n",
    "    \n",
    "    num_images = images.shape[0]\n",
    "    # Create a figure with two columns: one for image, one for mask.\n",
    "    fig, axes = plt.subplots(num_images, 2, figsize=(10, 5 * num_images))\n",
    "    \n",
    "    # If there's only one sample in the batch, make axes a list of two elements.\n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Denormalize the images for display\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Convert tensor to image format (H x W x C)\n",
    "        img = images[i].transpose(1, 2, 0)\n",
    "        # Denormalize\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Plot H&E image\n",
    "        axes[i][0].imshow(img)\n",
    "        axes[i][0].set_title(f\"H&E - {cell_type_names.get(cell_types[i], 'Unknown')}\")\n",
    "        axes[i][0].axis(\"off\")\n",
    "        \n",
    "        # Plot mask (using 'viridis' colormap)\n",
    "        axes[i][1].imshow(masks[i], cmap='viridis', vmin=0, vmax=4)\n",
    "        axes[i][1].set_title(\"Mask\")\n",
    "        axes[i][1].axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples from training set\n",
    "visualize_batch(train_loader, \"Training Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples from validation set\n",
    "visualize_batch(val_loader, \"Validation Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def export_sampled_images(dataset, output_root, split_name, create_zip=True):\n",
    "    \"\"\"\n",
    "    Copies H&E images and masks from the dataset.samples list to a structured folder:\n",
    "        output_root / split_name / {cell_type_name}/\n",
    "    Args:\n",
    "        dataset: A SegPathDataset instance (already constructed)\n",
    "        output_root: Base directory for exporting the images\n",
    "        split_name: 'train', 'val', or 'test' (used to create subfolders)\n",
    "        create_zip: Whether to create a zip file of the output directory\n",
    "    \"\"\"\n",
    "    print(f\"\\nExporting {len(dataset.samples)} {split_name} samples to {output_root} ...\")\n",
    "    \n",
    "    # Ensure the split subfolder exists\n",
    "    split_folder = os.path.join(output_root, split_name)\n",
    "    os.makedirs(split_folder, exist_ok=True)\n",
    "    \n",
    "    for (he_path, mask_path, label) in tqdm(dataset.samples):\n",
    "        # Convert numeric label back to cell type name\n",
    "        cell_type_name = [\n",
    "            k for k, v in dataset.cell_type_mapping.items() if v == label\n",
    "        ][0]\n",
    "        \n",
    "        # Make a subfolder for each cell type under train/val/test\n",
    "        cell_type_folder = os.path.join(split_folder, cell_type_name)\n",
    "        os.makedirs(cell_type_folder, exist_ok=True)\n",
    "        \n",
    "        # Copy the files\n",
    "        he_filename = os.path.basename(he_path)\n",
    "        mask_filename = os.path.basename(mask_path)\n",
    "        \n",
    "        # Destination paths\n",
    "        he_dest = os.path.join(cell_type_folder, he_filename)\n",
    "        mask_dest = os.path.join(cell_type_folder, mask_filename)\n",
    "        \n",
    "        # Copy the H&E and mask\n",
    "        shutil.copy2(he_path, he_dest)\n",
    "        shutil.copy2(mask_path, mask_dest)\n",
    "    \n",
    "    print(f\"Export complete for {split_name} set!\")\n",
    "    \n",
    "    # Create a zip file if requested\n",
    "    if create_zip:\n",
    "        print(f\"Creating zip file for {split_name} set...\")\n",
    "        zip_path = os.path.join(output_root, f\"segpath_{split_name}.zip\")\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(split_folder):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, output_root)\n",
    "                    zipf.write(file_path, arcname)\n",
    "        print(f\"Zip file created at: {zip_path}\")\n",
    "\n",
    "# Create the top-level 'sampled' directory somewhere you have write access\n",
    "sampled_dir = os.path.join(base_dir, \"sampled\")\n",
    "os.makedirs(sampled_dir, exist_ok=True)\n",
    "\n",
    "# Export train set\n",
    "export_sampled_images(train_dataset, sampled_dir, \"train\")\n",
    "\n",
    "# Export val set\n",
    "export_sampled_images(val_dataset, sampled_dir, \"val\")\n",
    "\n",
    "# Export test set\n",
    "export_sampled_images(test_dataset, sampled_dir, \"test\")\n",
    "\n",
    "# Optional: Create a single combined zip file with all splits\n",
    "print(\"\\nCreating combined zip file...\")\n",
    "combined_zip_path = os.path.join(base_dir, \"segpath_all_splits.zip\")\n",
    "with zipfile.ZipFile(combined_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, _, files in os.walk(sampled_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith('.zip'):  # Skip the individual zip files\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, os.path.dirname(sampled_dir))\n",
    "                zipf.write(file_path, arcname)\n",
    "print(f\"Combined zip file created at: {combined_zip_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Definition\n",
    "\n",
    "We'll use a UNet model from segmentation_models_pytorch with ResNet34 backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model will output 5 classes:\n",
    "# 0: background, 1: epithelial, 2: smooth muscle, 3: lymphocyte, 4: myeloid\n",
    "num_classes = 5\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",      # Upgrade from resnet34 to resnet50 for better feature extraction\n",
    "    encoder_weights=\"imagenet\",   # Use ImageNet pre-training\n",
    "    in_channels=3,                # RGB images\n",
    "    classes=num_classes,          # 5 classes (background + 4 cell types)\n",
    "    decoder_attention_type=\"scse\"  # Add spatial and channel squeeze & excitation for better feature refinement\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Helper Functions for Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred, target, n_classes=5):\n",
    "    \"\"\"Calculate mean IoU (Intersection over Union) for cell type classes\"\"\"\n",
    "    ious = []\n",
    "    \n",
    "    # Skip background class (0)\n",
    "    for cls in range(1, n_classes):\n",
    "        # Binary masks for this class\n",
    "        pred_binary = (pred == cls).astype(np.uint8)\n",
    "        target_binary = (target == cls).astype(np.uint8)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = np.logical_and(pred_binary, target_binary).sum()\n",
    "        union = np.logical_or(pred_binary, target_binary).sum()\n",
    "        \n",
    "        # Calculate IoU for this class\n",
    "        if union > 0:\n",
    "            iou = intersection / union\n",
    "        else:\n",
    "            # If this class doesn't appear in the ground truth or prediction\n",
    "            iou = 1.0 if intersection == 0 else 0.0\n",
    "        \n",
    "        ious.append(iou)\n",
    "    \n",
    "    # Return mean IoU\n",
    "    return np.mean(ious)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, test_loader, device, num_samples=4):\n",
    "    \"\"\"Visualize model predictions on test data\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch\n",
    "    batch = next(iter(test_loader))\n",
    "    images = batch['image'].to(device)\n",
    "    masks = batch['mask'].cpu().numpy()\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Convert images back for visualization\n",
    "    images = images.cpu().numpy()\n",
    "    \n",
    "    # Map class labels to colors for better visualization\n",
    "    colors = {\n",
    "        0: [0, 0, 0],       # Background: black\n",
    "        1: [255, 0, 0],     # Epithelial: red\n",
    "        2: [0, 255, 0],     # Smooth muscle: green\n",
    "        3: [0, 0, 255],     # Lymphocyte: blue\n",
    "        4: [255, 255, 0]    # Myeloid: yellow\n",
    "    }\n",
    "    \n",
    "    # Map class labels to names\n",
    "    class_names = {\n",
    "        0: 'Background',\n",
    "        1: 'Epithelial',\n",
    "        2: 'Smooth muscle',\n",
    "        3: 'Lymphocyte',\n",
    "        4: 'Myeloid'\n",
    "    }\n",
    "    \n",
    "    # Create a colormap function\n",
    "    def mask_to_rgb(mask):\n",
    "        rgb = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "        for i in range(5):  # 5 classes including background\n",
    "            rgb[mask == i] = colors[i]\n",
    "        return rgb\n",
    "    \n",
    "    # Plot the results\n",
    "    n = min(num_samples, len(images))\n",
    "    plt.figure(figsize=(15, 5 * n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Get image, true mask, and predicted mask\n",
    "        img = images[i].transpose(1, 2, 0)\n",
    "        true_mask = masks[i]\n",
    "        pred_mask = preds[i]\n",
    "        \n",
    "        # Denormalize image\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Convert masks to RGB\n",
    "        true_mask_rgb = mask_to_rgb(true_mask)\n",
    "        pred_mask_rgb = mask_to_rgb(pred_mask)\n",
    "        \n",
    "        # Calculate IoU\n",
    "        iou = calculate_iou(pred_mask, true_mask)\n",
    "        \n",
    "        # Get true cell type\n",
    "        cell_type = batch['cell_type'][i].item()\n",
    "        cell_type_name = class_names[cell_type]\n",
    "        \n",
    "        # Plot\n",
    "        plt.subplot(n, 3, i*3 + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'H&E Image - {cell_type_name}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(n, 3, i*3 + 2)\n",
    "        plt.imshow(true_mask_rgb)\n",
    "        plt.title('True Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(n, 3, i*3 + 3)\n",
    "        plt.imshow(pred_mask_rgb)\n",
    "        plt.title(f'Predicted Mask (IoU: {iou:.2f})')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot Training History Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot validation IoU\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['val_iou'], 'g-', label='Validation IoU')\n",
    "    plt.title('Validation IoU')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, weight=None, ce_weight=0.7, dice_weight=0.3):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "        self.dice = DiceLoss(mode='multiclass')\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.num_classes = 5  # background + 4 cell types\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        # Cross entropy loss\n",
    "        ce_loss = self.ce(outputs, targets)\n",
    "        \n",
    "        # For Dice loss, we need to convert targets to one-hot encoding\n",
    "        # First, ensure targets are long type\n",
    "        targets_long = targets.long()\n",
    "        \n",
    "        # One-hot encode the targets\n",
    "        targets_one_hot = F.one_hot(targets_long, num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Calculate Dice loss\n",
    "        dice_loss = self.dice(outputs, targets_one_hot)\n",
    "        \n",
    "        # Combine losses\n",
    "        return self.ce_weight * ce_loss + self.dice_weight * dice_loss\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=10, learning_rate=1e-4, class_weights=None):\n",
    "    \"\"\"Train the segmentation model with improved learning strategies\"\"\"\n",
    "    # Calculate class weights based on dataset distribution\n",
    "    class_samples = {\n",
    "        'background': 1000000,  # Estimate for background pixels\n",
    "        'epithelial': 53018,\n",
    "        'smooth_muscle': 62356,\n",
    "        'lymphocyte': 24546,\n",
    "        'myeloid': 28270\n",
    "    }\n",
    "    \n",
    "    # Calculate weights (inversely proportional to frequency)\n",
    "    if class_weights is None:\n",
    "        num_classes = 5  # background + 4 cell types\n",
    "        class_weights = [1.0] * num_classes\n",
    "        for i, class_name in enumerate(['background', 'epithelial', 'smooth_muscle', 'lymphocyte', 'myeloid']):\n",
    "            if i > 0:  # Skip background or assign small weight\n",
    "                class_weights[i] = max(class_samples.values()) / class_samples[class_name]\n",
    "        \n",
    "        # Normalize weights\n",
    "        total = sum(class_weights)\n",
    "        class_weights = [w/total*len(class_weights) for w in class_weights]\n",
    "    \n",
    "    # Convert to tensor and move to device\n",
    "    class_weights_tensor = torch.tensor(class_weights).float().to(device)\n",
    "    print(\"Class weights:\", class_weights)\n",
    "    \n",
    "    # Use standard CrossEntropyLoss with class weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    # Optimizer with weight decay for regularization\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Initialize training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_iou': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    # Track best model\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    \n",
    "    # Early stopping parameters with increased patience\n",
    "    patience = 10  # Increased from 5\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    # Start training\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
    "        \n",
    "        for batch in train_pbar:\n",
    "            # Move batch to device\n",
    "            images = batch['image'].to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Calculate epoch loss\n",
    "        epoch_train_loss = train_loss / len(train_loader.dataset)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_iou_scores = []\n",
    "        class_ious = {i: [] for i in range(1, 5)}  # Track IoU for each class separately\n",
    "        \n",
    "        # Progress bar for validation\n",
    "        val_pbar = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                # Move batch to device\n",
    "                images = batch['image'].to(device)\n",
    "                masks = batch['mask'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                # Update statistics\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # Calculate IoU for each image in batch\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                for i in range(preds.size(0)):\n",
    "                    # Calculate mean IoU across all classes\n",
    "                    iou = calculate_iou(preds[i].cpu().numpy(), masks[i].cpu().numpy())\n",
    "                    val_iou_scores.append(iou)\n",
    "                    \n",
    "                    # Calculate per-class IoU\n",
    "                    for cls in range(1, 5):  # Skip background\n",
    "                        # Create binary masks for this class\n",
    "                        pred_binary = (preds[i].cpu().numpy() == cls).astype(np.uint8)\n",
    "                        target_binary = (masks[i].cpu().numpy() == cls).astype(np.uint8)\n",
    "                        \n",
    "                        # Calculate intersection and union\n",
    "                        intersection = np.logical_and(pred_binary, target_binary).sum()\n",
    "                        union = np.logical_or(pred_binary, target_binary).sum()\n",
    "                        \n",
    "                        # Calculate IoU for this class\n",
    "                        if union > 0:\n",
    "                            cls_iou = intersection / union\n",
    "                        else:\n",
    "                            # If this class doesn't appear in the ground truth or prediction\n",
    "                            cls_iou = 1.0 if intersection == 0 else 0.0\n",
    "                        \n",
    "                        class_ious[cls].append(cls_iou)\n",
    "                \n",
    "                val_pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        epoch_val_iou = np.mean(val_iou_scores)\n",
    "        \n",
    "        # Update learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_iou'].append(epoch_val_iou)\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | Val IoU: {epoch_val_iou:.4f} | LR: {current_lr:.2e}')\n",
    "        \n",
    "        # Print class-specific IoUs\n",
    "        for cls, ious in class_ious.items():\n",
    "            if ious:  # Check if we have any valid IoUs for this class\n",
    "                cls_name = ['', 'Epithelial', 'Smooth Muscle', 'Lymphocyte', 'Myeloid'][cls]\n",
    "                cls_iou = np.mean(ious)\n",
    "                print(f'  {cls_name} IoU: {cls_iou:.4f}')\n",
    "        \n",
    "        # Save best model and check for early stopping\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            no_improve_epochs = 0\n",
    "            print(f'New best model saved! (Val Loss: {best_val_loss:.4f})')\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f'Early stopping after {epoch+1} epochs!')\n",
    "            break\n",
    "    \n",
    "    # Load best model weights\n",
    "    print(f'Training complete. Best Val Loss: {best_val_loss:.4f}')\n",
    "    if best_model_weights:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Helper function for per-class IoU\n",
    "def calculate_class_iou(pred, target, cls):\n",
    "    \"\"\"Calculate IoU for a specific class\"\"\"\n",
    "    # Create binary masks for this class\n",
    "    pred_binary = (pred == cls).astype(np.uint8)\n",
    "    target_binary = (target == cls).astype(np.uint8)\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = np.logical_and(pred_binary, target_binary).sum()\n",
    "    union = np.logical_or(pred_binary, target_binary).sum()\n",
    "    \n",
    "    # Calculate IoU for this class\n",
    "    if union > 0:\n",
    "        iou = intersection / union\n",
    "    else:\n",
    "        # If this class doesn't appear in the ground truth or prediction\n",
    "        iou = 1.0 if intersection == 0 else 0.0\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run the Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training process\n",
    "def train_segmentation_model(model, train_loader, val_loader, test_loader, device, \n",
    "                           num_epochs=10, learning_rate=1e-4, output_dir='model_output'):\n",
    "    \"\"\"Run the full training process and evaluate the model\"\"\"\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    \n",
    "    # Save the trained model\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model_path = os.path.join(output_dir, 'segpath_model.pth')\n",
    "    torch.save(trained_model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Visualize predictions on test set\n",
    "    print(\"Visualizing predictions on test set...\")\n",
    "    visualize_predictions(trained_model, test_loader, device)\n",
    "    \n",
    "    return trained_model\n",
    "\n",
    "# Start the training process\n",
    "trained_model = train_segmentation_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    device=device,\n",
    "    num_epochs=10,  # Start with 10 epochs\n",
    "    learning_rate=1e-4,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Apply Trained Model to New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_new_image(model, image_path, device):\n",
    "    \"\"\"Apply the trained model to a new image\"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image at {image_path}\")\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get original dimensions\n",
    "    h, w, _ = image.shape\n",
    "    print(f\"Original image size: {h}x{w}\")\n",
    "    \n",
    "    # Calculate padding needed to make dimensions divisible by 32\n",
    "    new_h = ((h + 31) // 32) * 32\n",
    "    new_w = ((w + 31) // 32) * 32\n",
    "    \n",
    "    # Apply transforms with padding\n",
    "    transform = A.Compose([\n",
    "        A.PadIfNeeded(min_height=new_h, min_width=new_w, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0)),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    # Transform the image\n",
    "    transformed = transform(image=image)\n",
    "    image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    # Check if dimensions are suitable\n",
    "    print(f\"Padded tensor size: {image_tensor.shape}\")\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            output = model(image_tensor)\n",
    "            pred = torch.argmax(output, dim=1).cpu().numpy()[0]\n",
    "            # Crop back to original size\n",
    "            pred = pred[:h, :w]\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e) or \"MPS out of memory\" in str(e):\n",
    "                print(\"Image too large for direct processing, use apply_to_large_image instead\")\n",
    "                raise e\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # Map class labels to colors\n",
    "    colors = {\n",
    "        0: [0, 0, 0],       # Background: black\n",
    "        1: [255, 0, 0],     # Epithelial: red\n",
    "        2: [0, 255, 0],     # Smooth muscle: green\n",
    "        3: [0, 0, 255],     # Lymphocyte: blue\n",
    "        4: [255, 255, 0]    # Myeloid: yellow\n",
    "    }\n",
    "    \n",
    "    # Create RGB mask\n",
    "    pred_rgb = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n",
    "    for i in range(5):\n",
    "        pred_rgb[pred == i] = colors[i]\n",
    "    \n",
    "    # Display the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(pred_rgb)\n",
    "    plt.title('Predicted Segmentation')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_on_random_samples(model, base_dir, device, num_samples=5):\n",
    "    \"\"\"Test the model on random samples from the SegPath dataset folders\"\"\"\n",
    "    # Define the cell type folders\n",
    "    cell_type_dirs = {\n",
    "        'epithelial': os.path.join(base_dir, 'panCK_Epithelium'),\n",
    "        'smooth_muscle': os.path.join(base_dir, 'aSMA_SmoothMuscle'),\n",
    "        'lymphocyte': os.path.join(base_dir, 'CD3CD20_Lymphocyte'),\n",
    "        'myeloid': os.path.join(base_dir, 'MNDA_MyeloidCell')\n",
    "    }\n",
    "    \n",
    "    # Get all H&E image paths\n",
    "    he_images = []\n",
    "    for cell_type, directory in cell_type_dirs.items():\n",
    "        if not os.path.exists(directory):\n",
    "            continue\n",
    "        \n",
    "        # Find all HE images in this directory\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('_HE.png'):\n",
    "                he_path = os.path.join(directory, filename)\n",
    "                mask_path = os.path.join(directory, filename.replace('_HE.png', '_mask.png'))\n",
    "                \n",
    "                if os.path.exists(mask_path):\n",
    "                    # Store the image path, mask path, and cell type\n",
    "                    he_images.append((he_path, mask_path, cell_type))\n",
    "    \n",
    "    print(f\"Found {len(he_images)} image pairs across all cell types\")\n",
    "    \n",
    "    # Randomly select samples\n",
    "    import random\n",
    "    random.seed(42)  # For reproducibility\n",
    "    if len(he_images) < num_samples:\n",
    "        selected_samples = he_images\n",
    "    else:\n",
    "        selected_samples = random.sample(he_images, num_samples)\n",
    "    \n",
    "    # Process each selected sample\n",
    "    for idx, (img_path, mask_path, cell_type) in enumerate(selected_samples):\n",
    "        print(f\"\\nSample {idx+1}/{len(selected_samples)}: {cell_type}\")\n",
    "        print(f\"Image: {os.path.basename(img_path)}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the ground truth mask\n",
    "            ground_truth = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Apply the model to get prediction\n",
    "            prediction = apply_to_new_image(model, img_path, device)\n",
    "            \n",
    "            # Load the original image for visualization\n",
    "            original = cv2.imread(img_path)\n",
    "            original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Calculate IoU for this sample's cell type\n",
    "            cell_type_label = {\n",
    "                'epithelial': 1,\n",
    "                'smooth_muscle': 2,\n",
    "                'lymphocyte': 3, \n",
    "                'myeloid': 4\n",
    "            }[cell_type]\n",
    "            \n",
    "            # Prepare binary masks for IoU calculation\n",
    "            gt_binary = (ground_truth == 1).astype(np.uint8)  # In ground truth, 1 indicates the target cell\n",
    "            pred_binary = (prediction == cell_type_label).astype(np.uint8)  # In prediction, we look for the specific cell type\n",
    "            \n",
    "            # Calculate IoU\n",
    "            intersection = np.logical_and(gt_binary, pred_binary).sum()\n",
    "            union = np.logical_or(gt_binary, pred_binary).sum()\n",
    "            if union > 0:\n",
    "                iou = intersection / union\n",
    "            else:\n",
    "                iou = 0.0\n",
    "            \n",
    "            print(f\"IoU score: {iou:.4f}\")\n",
    "            \n",
    "            # Visualize original, ground truth, and prediction\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(original)\n",
    "            plt.title('Original H&E')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(gt_binary, cmap='gray')\n",
    "            plt.title(f'Ground Truth ({cell_type})')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            # Use a colormap for prediction\n",
    "            colors = np.zeros((prediction.shape[0], prediction.shape[1], 3), dtype=np.uint8)\n",
    "            for i in range(5):  # 5 classes including background\n",
    "                if i == 0:  # Background\n",
    "                    colors[prediction == i] = [0, 0, 0]  # Black\n",
    "                elif i == cell_type_label:  # Target cell type\n",
    "                    colors[prediction == i] = [255, 0, 0]  # Red\n",
    "                else:  # Other cell types\n",
    "                    colors[prediction == i] = [0, 255, 0]  # Green\n",
    "            \n",
    "            plt.imshow(colors)\n",
    "            plt.title(f'Prediction (IoU: {iou:.2f})')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {str(e)}\")\n",
    "    \n",
    "    return\n",
    "\n",
    "# Example usage\n",
    "test_model_on_random_samples(trained_model, base_dir, device, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Apply to Visium HD Images\n",
    "\n",
    "This section shows how to use the trained model to segment cell types in Visium HD bladder tissue images.\n",
    "Here we'll implement tiling for large images and mapping segmentation results to Visium spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tma_with_different_scales(model, image_path, device, scales=[1.0, 0.5, 0.25]):\n",
    "    \"\"\"Test the model with different image scales to find the best match for TMA images\"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Could not load image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Take a center crop to speed up processing\n",
    "    h, w = image.shape[:2]\n",
    "    center_y, center_x = h // 2, w // 2\n",
    "    crop_size = 2000  # Larger crop to allow for scaling\n",
    "    crop = image[\n",
    "        max(0, center_y - crop_size//2):min(h, center_y + crop_size//2),\n",
    "        max(0, center_x - crop_size//2):min(w, center_x + crop_size//2)\n",
    "    ]\n",
    "    \n",
    "    # Display the original crop\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(crop)\n",
    "    plt.title(f\"Original Image Crop from {os.path.basename(image_path)}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    class_names = ['Background', 'Epithelial', 'Smooth Muscle', 'Lymphocyte', 'Myeloid']\n",
    "    colors = {\n",
    "        0: [0, 0, 0],       # Background: black\n",
    "        1: [255, 0, 0],     # Epithelial: red\n",
    "        2: [0, 255, 0],     # Smooth muscle: green\n",
    "        3: [0, 0, 255],     # Lymphocyte: blue\n",
    "        4: [255, 255, 0]    # Myeloid: yellow\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    best_scale = None\n",
    "    best_non_bg_percentage = 0\n",
    "    \n",
    "    for scale in scales:\n",
    "        print(f\"\\nTesting scale: {scale:.2f}\")\n",
    "        \n",
    "        # Resize the image according to scale\n",
    "        if scale != 1.0:\n",
    "            scaled_image = cv2.resize(crop, None, fx=scale, fy=scale)\n",
    "        else:\n",
    "            scaled_image = crop.copy()\n",
    "        \n",
    "        # Display scaled image\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(scaled_image)\n",
    "        plt.title(f\"Scale: {scale:.2f}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Make dimensions divisible by 32\n",
    "        h, w = scaled_image.shape[:2]\n",
    "        new_h = ((h + 31) // 32) * 32\n",
    "        new_w = ((w + 31) // 32) * 32\n",
    "        \n",
    "        # Apply transforms\n",
    "        transform = A.Compose([\n",
    "            A.PadIfNeeded(min_height=new_h, min_width=new_w, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0)),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        transformed = transform(image=scaled_image)\n",
    "        image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get model output\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "        \n",
    "        # Get raw probabilities for all classes\n",
    "        probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Visualize probability maps for each class\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i in range(5):\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            plt.imshow(probs[i, :scaled_image.shape[0], :scaled_image.shape[1]], cmap='viridis', vmin=0, vmax=1)\n",
    "            plt.title(f'{class_names[i]} Probability')\n",
    "            plt.colorbar()\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Get prediction (argmax)\n",
    "        pred = np.argmax(probs, axis=0)[:h, :w]\n",
    "        \n",
    "        # Count predictions\n",
    "        unique, counts = np.unique(pred, return_counts=True)\n",
    "        class_counts = {class_names[i]: 0 for i in range(5)}\n",
    "        for cls, count in zip(unique, counts):\n",
    "            class_counts[class_names[cls]] = count\n",
    "        \n",
    "        # Calculate total non-background percentage\n",
    "        total_pixels = pred.size\n",
    "        bg_pixels = class_counts['Background']\n",
    "        non_bg_pixels = total_pixels - bg_pixels\n",
    "        non_bg_percentage = (non_bg_pixels / total_pixels) * 100\n",
    "        \n",
    "        # Check if this is the best scale so far (most non-background pixels)\n",
    "        if non_bg_percentage > best_non_bg_percentage:\n",
    "            best_non_bg_percentage = non_bg_percentage\n",
    "            best_scale = scale\n",
    "        \n",
    "        # Print result\n",
    "        print(\"Class distribution:\")\n",
    "        for cls, count in class_counts.items():\n",
    "            percentage = (count / pred.size) * 100\n",
    "            print(f\"{cls}: {count} pixels ({percentage:.2f}%)\")\n",
    "        \n",
    "        # Create visualization mask\n",
    "        vis_mask = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n",
    "        for i in range(5):\n",
    "            vis_mask[pred == i] = colors[i]\n",
    "        \n",
    "        # Create overlay with original image\n",
    "        alpha = 0.5  # Transparency factor\n",
    "        overlay = scaled_image.copy()\n",
    "        for i in range(1, 5):  # Skip background\n",
    "            mask = pred == i\n",
    "            if np.any(mask):\n",
    "                overlay[mask] = alpha * overlay[mask] + (1 - alpha) * np.array(colors[i])\n",
    "        \n",
    "        # Create a figure with original, prediction, and overlay\n",
    "        plt.figure(figsize=(20, 7))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(scaled_image)\n",
    "        plt.title(f\"Scaled Image ({scale:.2f}x)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Segmentation mask\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(vis_mask)\n",
    "        plt.title(f\"Segmentation (Scale: {scale:.2f})\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Overlay with legend\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(overlay)\n",
    "        plt.title(f\"Overlay (Non-BG: {non_bg_percentage:.1f}%)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Create legend elements\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = []\n",
    "        for i in range(1, 5):  # Skip background\n",
    "            if i in unique:\n",
    "                # Convert RGB [0-255] to matplotlib's [0-1] scale\n",
    "                color_rgb = [c/255 for c in colors[i]]\n",
    "                percentage = (class_counts[class_names[i]] / pred.size) * 100\n",
    "                legend_elements.append(\n",
    "                    Patch(facecolor=color_rgb, \n",
    "                          label=f\"{class_names[i]} ({percentage:.1f}%)\")\n",
    "                )\n",
    "        \n",
    "        # Add legend\n",
    "        plt.figlegend(handles=legend_elements, loc='lower center', ncol=4, \n",
    "                     bbox_to_anchor=(0.5, -0.05), frameon=True, fancybox=True, shadow=True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.15)  # Make room for the legend\n",
    "        plt.show()\n",
    "        \n",
    "        # If any non-background classes are found, create a pie chart\n",
    "        if non_bg_pixels > 0:\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            \n",
    "            # Prepare data for pie chart (excluding background)\n",
    "            non_bg_counts = {k: v for k, v in class_counts.items() if k != 'Background' and v > 0}\n",
    "            \n",
    "            if non_bg_counts:\n",
    "                labels = list(non_bg_counts.keys())\n",
    "                sizes = list(non_bg_counts.values())\n",
    "                \n",
    "                # Corresponding colors for the pie chart\n",
    "                pie_colors = []\n",
    "                for label in labels:\n",
    "                    idx = class_names.index(label)\n",
    "                    color_rgb = [c/255 for c in colors[idx]]\n",
    "                    pie_colors.append(color_rgb)\n",
    "                \n",
    "                # Create pie chart\n",
    "                plt.pie(sizes, labels=labels, colors=pie_colors, autopct='%1.1f%%', \n",
    "                       shadow=True, startangle=90)\n",
    "                plt.axis('equal')  # Equal aspect ratio ensures pie is drawn as a circle\n",
    "                plt.title(f\"Cell Type Distribution at Scale {scale:.2f} (excluding background)\")\n",
    "                plt.show()\n",
    "        \n",
    "        # Store results\n",
    "        results[scale] = {\n",
    "            'class_counts': class_counts,\n",
    "            'prediction': pred,\n",
    "            'non_bg_percentage': non_bg_percentage\n",
    "        }\n",
    "    \n",
    "    # Print summary and recommendation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SCALE COMPARISON SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for scale, result in results.items():\n",
    "        print(f\"\\nScale {scale:.2f}:\")\n",
    "        for cls, count in result['class_counts'].items():\n",
    "            percentage = (count / result['prediction'].size) * 100\n",
    "            print(f\"  {cls}: {percentage:.2f}%\")\n",
    "    \n",
    "    if best_scale is not None:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"RECOMMENDED SCALE: {best_scale:.2f}\")\n",
    "        print(f\"This scale provided the best cell detection with {best_non_bg_percentage:.2f}% non-background pixels\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    return results, best_scale\n",
    "\n",
    "# Try with different scales on a TMA image\n",
    "# def analyze_tma_sample(tma_folder, model, device, scales=[1.0, 0.5, 0.25, 0.1]):\n",
    "def analyze_tma_sample(tma_folder, model, device, scales=[1.0, 0.5, 0.25, 0.1]):\n",
    "    \"\"\"Analyze a TMA sample with different scales\"\"\"\n",
    "    # Path to high-resolution image\n",
    "    img_path = os.path.join(tma_folder, \"tissue_hires_image.png\")\n",
    "    \n",
    "    if os.path.exists(img_path):\n",
    "        print(f\"Analyzing TMA sample: {os.path.basename(tma_folder)}\")\n",
    "        scale_results, best_scale = test_tma_with_different_scales(\n",
    "            model, \n",
    "            img_path, \n",
    "            device, \n",
    "            scales=scales\n",
    "        )\n",
    "        return {\n",
    "            'tma_folder': tma_folder,\n",
    "            'best_scale': best_scale,\n",
    "            'results': scale_results\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Image not found: {img_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure plots are displayed in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Find all TMA folders\n",
    "tma_folders = []\n",
    "for folder in os.listdir():\n",
    "    if folder.startswith(\"TMA_\") and os.path.isdir(folder):\n",
    "        tma_folders.append(folder)\n",
    "\n",
    "if not tma_folders:\n",
    "    print(\"No TMA folders found in the current directory\")\n",
    "else:\n",
    "    print(f\"Found {len(tma_folders)} TMA folders:\")\n",
    "    for folder in tma_folders:\n",
    "        print(f\"  - {folder}\")\n",
    "    \n",
    "    # Analyze each TMA folder\n",
    "    all_results = {}\n",
    "    \n",
    "    for tma_folder in tma_folders:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ANALYZING {tma_folder}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Analyze this TMA folder\n",
    "        result = analyze_tma_sample(\n",
    "            tma_folder=tma_folder,\n",
    "            model=trained_model,\n",
    "            device=device,\n",
    "            scales=[0.5, 0.25, 0.1]  # Use fewer scales to save time\n",
    "        )\n",
    "        \n",
    "        if result:\n",
    "            all_results[tma_folder] = result\n",
    "    \n",
    "    # Print summary of best scales\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY OF BEST SCALES FOR ALL TMA FOLDERS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for tma_folder, result in all_results.items():\n",
    "        best_scale = result['best_scale']\n",
    "        if best_scale is not None:\n",
    "            best_result = result['results'][best_scale]\n",
    "            non_bg_percent = best_result['non_bg_percentage']\n",
    "            print(f\"{tma_folder}: Best scale = {best_scale:.2f} (Non-background: {non_bg_percent:.2f}%)\")\n",
    "            \n",
    "            # Print cell type breakdown at the best scale\n",
    "            for cls, count in best_result['class_counts'].items():\n",
    "                if cls != 'Background':\n",
    "                    percentage = (count / best_result['prediction'].size) * 100\n",
    "                    if percentage > 0:\n",
    "                        print(f\"  {cls}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_svs_with_model(model, svs_path, device, patch_size=(984, 984), level=0):\n",
    "    \"\"\"\n",
    "    Extract a patch from an SVS file and analyze it with the segmentation model.\n",
    "    Will display results directly in the notebook.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    import albumentations as A\n",
    "    from albumentations.pytorch import ToTensorV2\n",
    "    import cv2\n",
    "    import openslide\n",
    "    from matplotlib.patches import Patch\n",
    "    \n",
    "    print(f\"Processing {svs_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Open the slide\n",
    "        slide = openslide.OpenSlide(str(svs_path))\n",
    "        \n",
    "        # Get slide information\n",
    "        dimensions = slide.level_dimensions\n",
    "        downsamplings = slide.level_downsamples\n",
    "        \n",
    "        print(f\"\\nSlide Information:\")\n",
    "        print(f\"Number of levels: {slide.level_count}\")\n",
    "        \n",
    "        for i in range(slide.level_count):\n",
    "            level_dim = dimensions[i]\n",
    "            level_downsample = downsamplings[i]\n",
    "            print(f\"  Level {i}: {level_dim[0]} x {level_dim[1]}, downsample: {level_downsample:.2f}x\")\n",
    "        \n",
    "        # If level is too high, adjust it\n",
    "        if level >= slide.level_count:\n",
    "            level = slide.level_count - 1\n",
    "            print(f\"Requested level too high, using level {level} instead\")\n",
    "            \n",
    "        # Get level dimensions\n",
    "        level_dims = dimensions[level]\n",
    "        \n",
    "        # Calculate the region to extract (center of the slide)\n",
    "        x = (level_dims[0] - patch_size[0]) // 2\n",
    "        y = (level_dims[1] - patch_size[1]) // 2\n",
    "        \n",
    "        # Ensure coordinates are non-negative\n",
    "        x, y = max(0, x), max(0, y)\n",
    "        \n",
    "        # Read the region\n",
    "        region = slide.read_region(\n",
    "            (x * int(downsamplings[level]), y * int(downsamplings[level])), \n",
    "            level, \n",
    "            patch_size\n",
    "        )\n",
    "        \n",
    "        # Convert to RGB (remove alpha channel)\n",
    "        patch = np.array(region.convert('RGB'))\n",
    "        \n",
    "        # Display the extracted patch\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(patch)\n",
    "        plt.title(f\"Extracted Patch - Level {level}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()  # Explicitly show the plot\n",
    "        \n",
    "        # Close the slide\n",
    "        slide.close()\n",
    "        \n",
    "        # Make dimensions divisible by 32 (required by U-Net)\n",
    "        h, w = patch.shape[:2]\n",
    "        new_h = ((h + 31) // 32) * 32\n",
    "        new_w = ((w + 31) // 32) * 32\n",
    "        \n",
    "        # Apply transforms\n",
    "        transform = A.Compose([\n",
    "            A.PadIfNeeded(min_height=new_h, min_width=new_w, border_mode=cv2.BORDER_CONSTANT, value=(0, 0, 0)),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        print(\"Applying model...\")\n",
    "        \n",
    "        transformed = transform(image=patch)\n",
    "        image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get model output\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "        \n",
    "        # Get probabilities and predictions\n",
    "        probs = torch.softmax(output, dim=1).cpu().numpy()[0]\n",
    "        pred = np.argmax(probs, axis=0)[:h, :w]\n",
    "        \n",
    "        # Define cell type colors and names\n",
    "        colors = {\n",
    "            0: [0, 0, 0],       # Background: black\n",
    "            1: [255, 0, 0],     # Epithelial: red\n",
    "            2: [0, 255, 0],     # Smooth muscle: green\n",
    "            3: [0, 0, 255],     # Lymphocyte: blue\n",
    "            4: [255, 255, 0]    # Myeloid: yellow\n",
    "        }\n",
    "        \n",
    "        class_names = ['Background', 'Epithelial', 'Smooth Muscle', 'Lymphocyte', 'Myeloid']\n",
    "        \n",
    "        # Count predictions\n",
    "        unique, counts = np.unique(pred, return_counts=True)\n",
    "        class_counts = {class_names[i]: 0 for i in range(5)}\n",
    "        for cls, count in zip(unique, counts):\n",
    "            class_counts[class_names[cls]] = count\n",
    "        \n",
    "        # Print result\n",
    "        print(\"\\nClass distribution:\")\n",
    "        for cls, count in class_counts.items():\n",
    "            percentage = (count / pred.size) * 100\n",
    "            print(f\"{cls}: {count} pixels ({percentage:.2f}%)\")\n",
    "        \n",
    "        # Visualize prediction\n",
    "        vis_mask = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n",
    "        for i in range(5):\n",
    "            vis_mask[pred == i] = colors[i]\n",
    "        \n",
    "        # Create overlay image\n",
    "        alpha = 0.5  # Transparency factor\n",
    "        overlay = patch.copy()\n",
    "        \n",
    "        # For non-background classes, apply color overlay\n",
    "        for i in range(1, 5):  # Skip background (0)\n",
    "            mask = pred == i\n",
    "            if np.any(mask):\n",
    "                overlay[mask] = alpha * overlay[mask] + (1 - alpha) * np.array(colors[i])\n",
    "        \n",
    "        # Display all three visualizations together\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(patch)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Pure segmentation\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(vis_mask)\n",
    "        plt.title(\"Cell Type Segmentation\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Overlay with legend\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(overlay)\n",
    "        plt.title(\"Segmentation Overlay\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Create legend elements\n",
    "        legend_elements = []\n",
    "        for i in range(1, 5):  # Skip background\n",
    "            if i in unique:\n",
    "                # Convert RGB [0-255] to matplotlib's [0-1] scale\n",
    "                color_rgb = [c/255 for c in colors[i]]\n",
    "                legend_elements.append(\n",
    "                    Patch(facecolor=color_rgb, \n",
    "                          label=f\"{class_names[i]} ({class_counts[class_names[i]]/pred.size*100:.1f}%)\")\n",
    "                )\n",
    "        \n",
    "        # Add legend to the figure\n",
    "        plt.figlegend(handles=legend_elements, loc='lower center', ncol=4, \n",
    "                      bbox_to_anchor=(0.5, 0.01), frameon=True, fancybox=True, shadow=True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.15)  # Make room for the legend\n",
    "        plt.show()  # Explicitly show the plot\n",
    "        \n",
    "        # Create a pie chart of cell type distribution (excluding background)\n",
    "        if any(class_counts[class_names[i]] > 0 for i in range(1, 5)):\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            \n",
    "            # Prepare data for pie chart (excluding background)\n",
    "            non_bg_counts = {k: v for k, v in class_counts.items() if k != 'Background' and v > 0}\n",
    "            \n",
    "            if non_bg_counts:  # Only create pie chart if there are non-background classes\n",
    "                labels = list(non_bg_counts.keys())\n",
    "                sizes = list(non_bg_counts.values())\n",
    "                \n",
    "                # Corresponding colors for the pie chart\n",
    "                pie_colors = []\n",
    "                for label in labels:\n",
    "                    idx = class_names.index(label)\n",
    "                    color_rgb = [c/255 for c in colors[idx]]\n",
    "                    pie_colors.append(color_rgb)\n",
    "                \n",
    "                # Create pie chart\n",
    "                plt.pie(sizes, labels=labels, colors=pie_colors, autopct='%1.1f%%', \n",
    "                        shadow=True, startangle=90)\n",
    "                plt.axis('equal')  # Equal aspect ratio ensures pie is drawn as a circle\n",
    "                plt.title(\"Cell Type Distribution (excluding background)\")\n",
    "                plt.show()  # Explicitly show the plot\n",
    "        \n",
    "        return {\n",
    "            'patch': patch,\n",
    "            'prediction': pred,\n",
    "            'class_counts': class_counts\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing slide: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure plots display inline in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Find all SVS files in the folder\n",
    "svs_folder = \"H&E_Svs\"\n",
    "svs_files = list(Path(svs_folder).glob(\"*.svs\"))\n",
    "\n",
    "if not svs_files:\n",
    "    print(f\"No SVS files found in {svs_folder}\")\n",
    "else:\n",
    "    print(f\"Found {len(svs_files)} SVS files:\")\n",
    "    for i, file in enumerate(svs_files):\n",
    "        print(f\"{i+1}. {file.name}\")\n",
    "    \n",
    "    # Process each SVS file\n",
    "    results = {}\n",
    "    for svs_file in svs_files:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Analyzing {svs_file.name}...\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Try level 0 first\n",
    "        analysis = analyze_svs_with_model(\n",
    "            model=trained_model,\n",
    "            svs_path=svs_file,\n",
    "            device=device,\n",
    "            level=0  # Start with level 0\n",
    "        )\n",
    "        \n",
    "        if analysis:\n",
    "            results[svs_file.name] = analysis\n",
    "    \n",
    "    # Print summary of results\n",
    "    print(\"\\nSummary of cell type distributions across all slides:\")\n",
    "    for slide_name, result in results.items():\n",
    "        print(f\"\\n{slide_name}:\")\n",
    "        for cell_type, count in result['class_counts'].items():\n",
    "            percentage = (count / result['prediction'].size) * 100\n",
    "            print(f\"  {cell_type}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "1. Implemented a complete pipeline for segmenting cell types in H&E histology images\n",
    "2. Trained a UNet model with ResNet34 backbone on the SegPath dataset\n",
    "3. Developed methods to apply the trained model to Visium HD spatial transcriptomics images\n",
    "4. Created tools for comparing segmentation-based cell type identification with gene expression data\n",
    "\n",
    "The trained model can be used to analyze the spatial distribution of cell types in tissue and correlate morphological features with gene expression patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final trained model\n",
    "if 'trained_model' in locals():\n",
    "    final_model_path = os.path.join(output_dir, 'final_segpath_model.pth')\n",
    "    torch.save(trained_model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved to: {final_model_path}\")\n",
    "    \n",
    "print(\"Notebook execution completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
